<!--Autonomous Navigation-->

<!DOCTYPE HTML>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <!-- Include the smooth scrolling script -->
</head>
<body class="is-preload">

    <!-- Include the navigation section -->
    <style>
        /* Add some style to the nav element */
        
        nav {
          background-color: transparent; /* Change the background color to a light gray */
          border: 1px solid; /* Add a thin border around the nav element */
          margin: 0; /* Remove any margin around the nav element */
          padding: 10px; /* Add some padding inside the nav element */
        }
      
        /* Add some style to the ul element */
        ul {
          list-style-type: none; /* Remove the bullet points from the list */
          margin: 0; /* Remove any margin around the list */
          padding: 0; /* Remove any padding around the list */
          display: flex; /* Make the list items display horizontally */
          justify-content: space-around; /* Distribute the list items evenly */
        }
      
        /* Add some style to the a elements */
        a {
          text-decoration: none; /* Remove the underline from the links */
          color: inherit; /* Change the link color to a dark gray */
          font-family: inherit, sans-serif; /* Change the font family to match the website */
          font-size: 24px; /* Change the font size to match the website */
        }
      
      </style>
    <!-- Navigation -->
    <nav>
        <ul class="links">
          <li><a href="index.html" class="button">Home</a></li>
          <li><a href="about.html" class="button">About</a></li>
          <li><a href="https://drive.google.com/file/d/1zedOrSw1yjuaD-fRH8E0ojYTnuCiH38W/view?usp=sharing" target="_blank" class="button">Resume</a></li>
        </ul>
      </nav>

    <!-- Include the header section -->

    <!-- Main content specific to project1.html -->
    
    <div id="main">
        <div class="box alt container">
          <p style="text-align: justify;">Skills: Pytorch, Matplotlib, NumPy</p>
					<header>
						<h2>AUTONOMOUS NAVIGATION: DAEPTH BASED PERCEPTION</h2>
					</header>
					<section>
						<header>
							<h3>Aim of the Project</h3>
							<!-- <p>This is the subtitle for this particular heading</p> -->
						</header>

						<p style="text-align: justify;">In autonomous driving, perception systems are pivotal as they interpret sensory data to 
              understand the envi ronment, which is essential for decision-making and planning. Ensuring the safety of these perception 
              systems is fundamental for achieving high-level autonomy, allowing us to confidently delegate driving and monitoring tasks
               to machines. This project focuses on enhancing the understanding and navigation capabil ities of self-driving robots through 
               sensor fusion and computervision techniques. 
              Specifically, it explores the depth based perception using ZED2 camera to improve autonomous driving perception.</p>
					</section>
                    <center><img src="images/Project0/RosmasterR2.jpeg" alt="txt_to_img" width="500" height="500"/></a></center>
					<br>

            <section>
              <header>
                <h3>Background</h3>
              </header>
            <p align="justify">
            Checkout the Github Profile to look for more detailed report - 
            <ul class="links">
              <li><a href="https://github.com/Urviskumar" >Github</a></li>
            </ul> <br>
            In this project, we utilized depth-based perception to en
             able autonomous navigation of the robot in an unfamiliar
             environment. The fusion of 2D LiDAR and depth camera
             sensors demanded substantial computational resources, leading
             to system throttle errors during the object detection task
             alone. In addition to object detection, we also maneuvered the
             Rosmaster R2 bot autonomously, detecting traffic signs such
             as 'Move', 'Turn', and 'Stop'. Depth cameras and traditional
             cameras play critical roles in mobile robot perception, pro
             viding 3D environmental information and facilitating vision
             guided navigation, respectively. Fig1 shows such example of
             the camera that we have used in this project. </p>	</section>
            
             <section>
              <header>
                <h3>Hardware and Software Setup</h3>
              </header>
            <p align="center"><img src="images/Project0/ZED2_Camera.jpeg" alt="txt_to_img" width="700" height="500"/></a></p>
            <center style="font-size: 0;">
              <video width="1100" height="720"  autoplay muted>
                <source src="images/Project0/Autonomous_Navigation.mp4" type="video/mp4">
              </video></center>
            
            
            <p align="justify">The project utilized a combination of advanced hardware
             and software to process and analyze sensor data:
            
            * Jetson Xavier Processor: Served as the computational
             backbone, handling data processing and model execution.
            * ZED2 RGBD Camera:Provided high-resolution images
             and depth data, crucial for object detection and distance
             estimation. Fig 2 shows such example of the camera that
             we have used in this project.
            * ROS (Robot Operating System): Enabled efficient sys
            tem integration, data handling, and algorithm implemen
            tation.
            * ZED SDK: Offered tools and APIs for extracting and
             processing data from the ZED 2 camera.
            
            The integration of these hardware components through ROS
             facilitated a modular approach, allowing for the independent
             development and testing of subsystems. Below figure shows the implemented environment we have used through this project.</p></section>
            
             <p align="center"><img src="images/Project0/our_environmnet.jpeg" alt="txt_to_img" width="700" height="500"/></a></p>
            
             <section>
              <header>
                <h3>Challenges and Solution</h3>
              </header>
            <p align="justify">The project faced significant challenges in terms of  computational power.
             The computational capacity of the Jetson
             Xavier was limited, which posed a significant hurdle. The
             team was unable to implement 2D LiDAR fusion and the
             ZED 2 camera as initially planned due to these constraints.</section>
            
            <section>
              <header>
                <h3>Working</h3>
              </header>
             <p align="justify">In this project, we embarked on a
             journey to develop an autonomous navigation system, starting with 
             the fundamental task of detecting lanes. We utilized the YOLOv5 pre-trained
             model for object detection. This model has been widely used
             in various applications, including lane detection, missing road
             lane markings detection, and pedestrian detection. The use of
             YOLOv5 allowed us to effectively detect objects in real-time,
             contributing significantly to the success of the project. Here we
             have detected successfully the 'Stop', 'Move', and 'Turn' signs
             using which robot will perform the task according to the signs. <br><br>

             <center style="font-size: 0;">
              <video width="1100" height="720"  autoplay muted>
                <source src="images/Project0/Depth_Based_Perception_Yolo_Detection.mp4" type="video/mp4">
              </video></center>